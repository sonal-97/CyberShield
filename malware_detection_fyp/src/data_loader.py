"""
CyberShield AI - Malware Detection Data Loader
Advanced data preprocessing and loading for malware classification.

Features:
- Binary visualization pattern analysis
- 26 malware class support (25 families + benign)
- Data augmentation and normalization
- Balanced dataset handling with class weights
"""

import os
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import cv2
from PIL import Image

class CorrectMalwareDataLoader:
    def __init__(self, data_dir="data", target_size=(256, 256), batch_size=32):
        """
        Initialize the correct malware data loader
        
        Args:
            data_dir: Directory containing malware dataset
            target_size: Target image size (width, height)
            batch_size: Batch size for training
        """
        self.data_dir = data_dir
        self.target_size = target_size
        self.batch_size = batch_size
        
        # Class mapping from reference implementation (EXACT match)
        self.class_index = {
            'Adialer.C': 0,
            'Agent.FYI': 1,
            'Allaple.A': 2,
            'Allaple.L': 3,
            'Alueron.gen!J': 4,
            'Autorun.K': 5,
            'Benign': 6,
            'C2LOP.P': 7,
            'C2LOP.gen!g': 8,
            'Dialplatform.B': 9,
            'Dontovo.A': 10,
            'Fakerean': 11,
            'Instantaccess': 12,
            'Lolyda.AA1': 13,
            'Lolyda.AA2': 14,
            'Lolyda.AA3': 15,
            'Lolyda.AT': 16,
            'Malex.gen!J': 17,
            'Obfuscator.AD': 18,
            'Rbot!gen': 19,
            'Skintrim.N': 20,
            'Swizzor.gen!E': 21,
            'Swizzor.gen!I': 22,
            'VB.AT': 23,
            'Wintrim.BX': 24,
            'Yuner.A': 25
        }
        
        # Reverse mapping for predictions
        self.index_to_class = {v: k for k, v in self.class_index.items()}
        
        # Data generators (will be initialized later)
        self.train_generator = None
        self.val_generator = None
        self.test_generator = None
        
    def create_dataset_csv(self, malware_dir, benign_dir=None):
        """
        Create CSV files for train/val/test splits following reference approach
        
        Args:
            malware_dir: Directory containing malware family folders
            benign_dir: Directory containing benign samples (optional)
        """
        print("Creating dataset CSV files...")
        
        all_data = []
        
        # Process all classes including benign
        for class_name in self.class_index.keys():
            if class_name == 'Benign':
                # Handle benign images
                if benign_dir and os.path.exists(benign_dir):
                    images = os.listdir(benign_dir)
                    for img in images:
                        if img.lower().endswith(('.png', '.jpg', '.jpeg')):
                            # Use relative path from dataset root
                            img_path = os.path.join('benign_imgs', img)
                            all_data.append([img_path, class_name])
                else:
                    print(f"Warning: Benign directory not found: {benign_dir}")
                continue
                
            # Handle malware families
            class_dir = os.path.join(malware_dir, class_name)
            if not os.path.exists(class_dir):
                print(f"Warning: {class_name} directory not found")
                continue
                
            images = os.listdir(class_dir)
            for img in images:
                if img.lower().endswith(('.png', '.jpg', '.jpeg')):
                    # Use relative path from dataset root (include malimg_dataset prefix)
                    img_path = os.path.join('malimg_dataset', class_name, img)
                    all_data.append([img_path, class_name])
        
        # Create DataFrame
        df = pd.DataFrame(all_data, columns=['img_code', 'target'])
        
        # Split dataset: 60% train, 20% val, 20% test
        train_df, temp_df = train_test_split(df, test_size=0.4, stratify=df['target'], random_state=42)
        val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['target'], random_state=42)
        
        # Save CSV files
        os.makedirs(self.data_dir, exist_ok=True)
        train_df.to_csv(os.path.join(self.data_dir, 'train_combined.csv'), index=False)
        val_df.to_csv(os.path.join(self.data_dir, 'val_combined.csv'), index=False)
        test_df.to_csv(os.path.join(self.data_dir, 'test_combined.csv'), index=False)
        
        print(f"Dataset created:")
        print(f"  Train: {len(train_df)} samples")
        print(f"  Validation: {len(val_df)} samples")
        print(f"  Test: {len(test_df)} samples")
        print(f"  Total classes: {len(df['target'].unique())}")
        
        return train_df, val_df, test_df
    
    def setup_generators(self, data_root_dir, use_augmentation=True):
        """
        Setup data generators following reference implementation approach
        
        Args:
            data_root_dir: Root directory containing the dataset
            use_augmentation: Whether to use data augmentation for training
        """
        print("Setting up data generators...")
        
        # Load CSV files
        train_df = pd.read_csv(os.path.join(self.data_dir, 'train_combined.csv'))
        val_df = pd.read_csv(os.path.join(self.data_dir, 'val_combined.csv'))
        test_df = pd.read_csv(os.path.join(self.data_dir, 'test_combined.csv'))
        
        # Data generator with proper normalization (following reference)
        datagen = ImageDataGenerator(rescale=1.0/255.0)
        
        # Training generator with strong augmentation to prevent overfitting
        if use_augmentation:
            train_datagen = ImageDataGenerator(
                rescale=1.0/255.0,
                rotation_range=30,  # Increased rotation
                width_shift_range=0.2,  # More shifting
                height_shift_range=0.2,
                horizontal_flip=True,
                vertical_flip=True,
                zoom_range=0.3,  # More zoom variation
                shear_range=0.2,  # More shearing
                brightness_range=[0.7, 1.3],  # Wider brightness range
                channel_shift_range=0.2,  # Add channel shifting
                fill_mode='nearest'
            )
        else:
            train_datagen = datagen
        
        # Create generators
        self.train_generator = train_datagen.flow_from_dataframe(
            dataframe=train_df,
            directory=data_root_dir,
            x_col="img_code",
            y_col="target",
            target_size=self.target_size,
            color_mode="rgb",  # Following reference (they use RGB but analyze grayscale patterns)
            batch_size=self.batch_size,
            class_mode="categorical",
            shuffle=True,
            seed=42
        )
        
        self.val_generator = datagen.flow_from_dataframe(
            dataframe=val_df,
            directory=data_root_dir,
            x_col="img_code",
            y_col="target",
            target_size=self.target_size,
            color_mode="rgb",
            batch_size=self.batch_size,
            class_mode="categorical",
            shuffle=False,
            seed=42
        )
        
        self.test_generator = datagen.flow_from_dataframe(
            dataframe=test_df,
            directory=data_root_dir,
            x_col="img_code",
            y_col="target",
            target_size=self.target_size,
            color_mode="rgb",
            batch_size=self.batch_size,
            class_mode="categorical",
            shuffle=False,
            seed=42
        )
        
        print(f"Generators created:")
        print(f"  Train samples: {self.train_generator.samples}")
        print(f"  Validation samples: {self.val_generator.samples}")
        print(f"  Test samples: {self.test_generator.samples}")
        print(f"  Number of classes: {len(self.train_generator.class_indices)}")
        
        return self.train_generator, self.val_generator, self.test_generator
    
    def get_class_weights(self):
        """Calculate class weights for imbalanced dataset"""
        if self.train_generator is None:
            raise ValueError("Generators not set up. Call setup_generators() first.")
        
        from sklearn.utils.class_weight import compute_class_weight
        
        # Get class distribution
        class_counts = np.bincount(self.train_generator.classes)
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(self.train_generator.classes),
            y=self.train_generator.classes
        )
        
        return dict(enumerate(class_weights))
    
    def preprocess_single_image(self, image_path):
        """
        Preprocess a single image for prediction
        
        Args:
            image_path: Path to the image file
            
        Returns:
            Preprocessed image array
        """
        # Load and resize image
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        # Resize to target size
        img = cv2.resize(img, self.target_size)
        
        # Normalize pixel values
        img = img.astype(np.float32) / 255.0
        
        # Add batch dimension
        img = np.expand_dims(img, axis=0)
        
        return img
    
    def analyze_image_properties(self, image_path):
        """
        Analyze image properties for debugging
        
        Args:
            image_path: Path to the image file
            
        Returns:
            Dictionary with image properties
        """
        img = cv2.imread(image_path)
        if img is None:
            return None
        
        # Convert to grayscale for analysis
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        properties = {
            'shape': img.shape,
            'dtype': img.dtype,
            'min_value': np.min(img),
            'max_value': np.max(img),
            'mean_value': np.mean(img),
            'std_value': np.std(img),
            'grayscale_entropy': self._calculate_entropy(gray),
            'unique_colors': len(np.unique(img.reshape(-1, img.shape[-1]), axis=0))
        }
        
        return properties
    
    def _calculate_entropy(self, image):
        """Calculate entropy of grayscale image"""
        hist, _ = np.histogram(image, bins=256, range=(0, 256))
        hist = hist[hist > 0]  # Remove zero entries
        prob = hist / np.sum(hist)
        entropy = -np.sum(prob * np.log2(prob))
        return entropy

if __name__ == "__main__":
    # Example usage
    loader = CorrectMalwareDataLoader()
    
    # Create dataset (if needed)
    # loader.create_dataset_csv("path/to/malware/families", "path/to/benign")
    
    # Setup generators
    # train_gen, val_gen, test_gen = loader.setup_generators("path/to/dataset/root")
    
    print("Correct malware data loader initialized successfully!")