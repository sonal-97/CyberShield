"""
Transfer Learning Model Builder for Malware Detection
Using pre-trained models for better feature extraction

Author: AI Assistant
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import (
    ResNet50, EfficientNetB0, MobileNetV2, VGG16
)
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

class TransferLearningModelBuilder:
    def __init__(self, input_shape=(224, 224, 3), num_classes=26):
        """
        Initialize transfer learning model builder
        
        Args:
            input_shape: Input image shape
            num_classes: Number of output classes
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        
    def build_transfer_model(self, base_model_name="efficientnet", freeze_base=True):
        """
        Build transfer learning model
        
        Args:
            base_model_name: Base model to use ('efficientnet', 'resnet50', 'mobilenet', 'vgg16')
            freeze_base: Whether to freeze base model weights initially
            
        Returns:
            Compiled transfer learning model
        """
        # Get base model
        if base_model_name.lower() == "efficientnet":
            base_model = EfficientNetB0(
                weights='imagenet',
                include_top=False,
                input_shape=self.input_shape
            )
        elif base_model_name.lower() == "resnet50":
            base_model = ResNet50(
                weights='imagenet',
                include_top=False,
                input_shape=self.input_shape
            )
        elif base_model_name.lower() == "mobilenet":
            base_model = MobileNetV2(
                weights='imagenet',
                include_top=False,
                input_shape=self.input_shape
            )
        elif base_model_name.lower() == "vgg16":
            base_model = VGG16(
                weights='imagenet',
                include_top=False,
                input_shape=self.input_shape
            )
        else:
            raise ValueError(f"Unsupported base model: {base_model_name}")
        
        # Freeze base model if requested
        if freeze_base:
            base_model.trainable = False
            print(f"Base model ({base_model_name}) frozen for initial training")
        else:
            print(f"Base model ({base_model_name}) unfrozen for fine-tuning")
        
        # Add custom classification head
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        
        # Dense layers with regularization
        x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.5)(x)
        
        x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.4)(x)
        
        x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        # Output layer
        predictions = Dense(self.num_classes, activation='softmax', name='predictions')(x)
        
        # Create model
        model = Model(inputs=base_model.input, outputs=predictions)
        
        return model, base_model
    
    def compile_model(self, model, learning_rate=0.001, fine_tuning=False):
        """
        Compile the transfer learning model
        
        Args:
            model: Model to compile
            learning_rate: Learning rate
            fine_tuning: Whether this is for fine-tuning (lower LR)
            
        Returns:
            Compiled model
        """
        if fine_tuning:
            # Lower learning rate for fine-tuning
            lr = learning_rate / 10
            print(f"Fine-tuning mode: Using lower learning rate {lr}")
        else:
            lr = learning_rate
            print(f"Initial training mode: Using learning rate {lr}")
        
        optimizer = Adam(
            learning_rate=lr,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def unfreeze_base_model(self, base_model, unfreeze_layers=50):
        """
        Unfreeze top layers of base model for fine-tuning
        
        Args:
            base_model: Base model to unfreeze
            unfreeze_layers: Number of top layers to unfreeze
        """
        base_model.trainable = True
        
        # Freeze bottom layers, unfreeze top layers
        for layer in base_model.layers[:-unfreeze_layers]:
            layer.trainable = False
        
        for layer in base_model.layers[-unfreeze_layers:]:
            layer.trainable = True
        
        print(f"Unfroze top {unfreeze_layers} layers for fine-tuning")
        print(f"Total trainable parameters: {sum([tf.keras.backend.count_params(w) for w in base_model.trainable_weights]):,}")
    
    def create_callbacks(self, model_save_path, fine_tuning=False):
        """
        Create callbacks for transfer learning
        
        Args:
            model_save_path: Path to save best model
            fine_tuning: Whether this is fine-tuning phase
            
        Returns:
            List of callbacks
        """
        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                model_save_path,
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,
                mode='max',
                verbose=1
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5 if not fine_tuning else 0.2,
                patience=3 if not fine_tuning else 2,
                min_lr=1e-8,
                verbose=1
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',
                patience=8 if not fine_tuning else 5,
                restore_best_weights=True,
                verbose=1,
                min_delta=0.001,
                mode='max'
            )
        ]
        
        return callbacks
    
    def get_model_summary(self, model):
        """Get model summary and parameter count"""
        print("Transfer Learning Model Summary:")
        print("=" * 50)
        model.summary()
        
        total_params = model.count_params()
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        non_trainable_params = total_params - trainable_params
        
        print(f"\nParameter Summary:")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Non-trainable parameters: {non_trainable_params:,}")
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'non_trainable_params': non_trainable_params
        }

if __name__ == "__main__":
    # Example usage
    builder = TransferLearningModelBuilder()
    model, base_model = builder.build_transfer_model("efficientnet")
    model = builder.compile_model(model)
    builder.get_model_summary(model)