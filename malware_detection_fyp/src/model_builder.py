"""
CyberShield AI - Neural Network Model Builder
Advanced CNN architectures for malware detection and classification.

Supports multiple model types:
- Standard CNN with regularization
- Deep CNN with advanced features
- Lightweight CNN for fast inference
- Regularized CNN for overfitting prevention
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Dropout, Flatten, Dense, 
    BatchNormalization, GlobalAveragePooling2D
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

class CorrectMalwareModelBuilder:
    def __init__(self, input_shape=(256, 256, 3), num_classes=26):
        """
        Initialize the correct model builder
        
        Args:
            input_shape: Input image shape (height, width, channels)
            num_classes: Number of malware classes (25 malware families + 1 benign)
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        
    def build_cnn_model(self, model_type="standard"):
        """
        Build CNN model following reference architecture
        
        Args:
            model_type: Type of model ("standard", "deep", "lightweight", "improved")
            
        Returns:
            Compiled Keras model
        """
        if model_type == "standard":
            return self._build_standard_cnn()
        elif model_type == "deep":
            return self._build_deep_cnn()
        elif model_type == "lightweight":
            return self._build_lightweight_cnn()
        elif model_type == "improved":
            return self._build_improved_cnn()
        elif model_type == "regularized":
            return self._build_regularized_cnn()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
    
    def _build_standard_cnn(self):
        """Build standard CNN architecture based on reference"""
        model = Sequential([
            # First convolutional block
            Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape,
                   kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Second convolutional block
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Third convolutional block
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Fourth convolutional block
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Global average pooling instead of flatten to reduce parameters
            GlobalAveragePooling2D(),
            
            # Dense layers
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),
            
            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),
            
            # Output layer
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])
        
        return model
    
    def _build_deep_cnn(self):
        """Build deeper CNN architecture for better feature extraction"""
        model = Sequential([
            # Block 1
            Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape,
                   kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.2),
            
            # Block 2
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.2),
            
            # Block 3
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.3),
            
            # Block 4
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.3),
            
            # Block 5
            Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.4),
            
            # Global average pooling
            GlobalAveragePooling2D(),
            
            # Dense layers
            Dense(1024, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),
            
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),
            
            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.5),
            
            # Output layer
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])
        
        return model
    
    def _build_lightweight_cnn(self):
        """Build lightweight CNN for faster training/inference"""
        model = Sequential([
            # Block 1
            Conv2D(16, (5, 5), activation='relu', input_shape=self.input_shape),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.2),
            
            # Block 2
            Conv2D(32, (5, 5), activation='relu'),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.2),
            
            # Block 3
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.3),
            
            # Block 4
            Conv2D(128, (3, 3), activation='relu'),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.3),
            
            # Global average pooling
            GlobalAveragePooling2D(),
            
            # Dense layers
            Dense(256, activation='relu'),
            Dropout(0.5),
            
            Dense(128, activation='relu'),
            Dropout(0.5),
            
            # Output layer
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])
        
        return model
    
    def _build_improved_cnn(self):
        """Build improved CNN architecture optimized for malware detection"""
        model = Sequential([
            # Block 1 - Initial feature extraction
            Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape,
                   kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.2),
            
            # Block 2 - Pattern recognition
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Block 3 - Complex pattern extraction
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            Conv2D(128, (1, 1), activation='relu', kernel_regularizer=l2(0.0005)),  # 1x1 conv for feature refinement
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.3),
            
            # Block 4 - High-level features
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            Conv2D(256, (1, 1), activation='relu', kernel_regularizer=l2(0.0005)),  # 1x1 conv for feature refinement
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.35),
            
            # Block 5 - Final feature extraction
            Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Conv2D(512, (3, 3), activation='relu', kernel_regularizer=l2(0.0005)),
            Dropout(0.4),
            
            # Global average pooling to reduce overfitting
            GlobalAveragePooling2D(),
            
            # Dense layers with progressive size reduction
            Dense(512, activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Dropout(0.5),
            
            Dense(256, activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Dropout(0.4),
            
            Dense(128, activation='relu', kernel_regularizer=l2(0.0005)),
            BatchNormalization(),
            Dropout(0.3),
            
            # Output layer
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])
        
        return model
    
    def _build_regularized_cnn(self):
        """Build heavily regularized CNN to prevent overfitting"""
        model = Sequential([
            # Block 1 - Smaller filters, more regularization
            Conv2D(16, (3, 3), activation='relu', input_shape=self.input_shape,
                   kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.3),
            MaxPooling2D(pool_size=(2, 2)),
            
            # Block 2
            Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.4),
            MaxPooling2D(pool_size=(2, 2)),
            
            # Block 3
            Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.4),
            MaxPooling2D(pool_size=(2, 2)),
            
            # Block 4
            Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.5),
            MaxPooling2D(pool_size=(2, 2)),
            
            # Global average pooling to reduce parameters
            GlobalAveragePooling2D(),
            
            # Minimal dense layers with heavy dropout
            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.6),
            
            Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
            BatchNormalization(),
            Dropout(0.5),
            
            # Output layer
            Dense(self.num_classes, activation='softmax', name='predictions')
        ])
        
        return model
    
    def compile_model(self, model, learning_rate=0.001, metrics=None):
        """
        Compile the model with improved optimizer and loss function
        
        Args:
            model: Keras model to compile
            learning_rate: Learning rate for optimizer
            metrics: List of metrics to track
            
        Returns:
            Compiled model
        """
        if metrics is None:
            metrics = ['accuracy', 'precision', 'recall']
        
        # Use Adam optimizer with improved settings
        optimizer = Adam(
            learning_rate=learning_rate,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7,
            clipnorm=1.0  # Gradient clipping to prevent exploding gradients
        )
        
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=metrics
        )
        
        return model
    
    def get_model_summary(self, model):
        """Get detailed model summary"""
        print("Model Architecture Summary:")
        print("=" * 50)
        model.summary()
        
        # Calculate total parameters
        total_params = model.count_params()
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        non_trainable_params = total_params - trainable_params
        
        print(f"\nParameter Summary:")
        print(f"Total parameters: {total_params:,}")
        print(f"Trainable parameters: {trainable_params:,}")
        print(f"Non-trainable parameters: {non_trainable_params:,}")
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'non_trainable_params': non_trainable_params
        }
    
    def create_callbacks(self, model_save_path="models/best_model.keras"):
        """
        Create improved training callbacks for better performance
        
        Args:
            model_save_path: Path to save the best model
            
        Returns:
            List of callbacks
        """
        callbacks = [
            # Save best model based on validation accuracy
            tf.keras.callbacks.ModelCheckpoint(
                model_save_path,
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,
                mode='max',
                verbose=1
            ),
            # Reduce learning rate when validation loss plateaus
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.3,  # More aggressive reduction
                patience=3,  # Shorter patience for faster adaptation
                min_lr=1e-8,
                verbose=1,
                cooldown=1
            ),
            # Early stopping to prevent overfitting
            tf.keras.callbacks.EarlyStopping(
                monitor='val_accuracy',  # Monitor validation accuracy instead
                patience=10,  # Increased patience for better convergence
                restore_best_weights=True,
                verbose=1,
                min_delta=0.005,  # Require more significant improvement
                mode='max'  # Maximize validation accuracy
            ),
            # Cosine annealing learning rate scheduler
            tf.keras.callbacks.LearningRateScheduler(
                lambda epoch: 0.0005 * (0.5 * (1 + np.cos(np.pi * epoch / 20))),
                verbose=0
            )
        ]
        
        return callbacks

if __name__ == "__main__":
    # Example usage
    builder = CorrectMalwareModelBuilder(input_shape=(256, 256, 3), num_classes=26)
    
    # Build and compile model
    model = builder.build_cnn_model(model_type="standard")
    model = builder.compile_model(model)
    
    # Get model summary
    builder.get_model_summary(model)
    
    print("Correct malware model builder initialized successfully!")